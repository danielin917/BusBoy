{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intrusion Detection Systems (IDS)\n",
    "\n",
    "- Anomaly detection or Outlier Detection is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset.\n",
    "\n",
    "- Intrusion Detection Systems are practical implementations of Anomaly Detectors that are intended to identify and later possibly act on any \"abnormal\" behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Vs. Unsupervised?\n",
    "\n",
    "- Broadly, there are two kinds of Machine Learning Algorithms: Supervised Methods and Unsupervised Methods.\n",
    "- The difference is that Supervised Methods require labels for the data while Unsupervised Methods do not.\n",
    "- In performing Anomaly Detection we generally do not have data from the anomalous class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning Algorithms for IDS \n",
    "\n",
    "Based on prior work (see Sabahar and Movaghar, 2008, IEEE for survey), these are some of the commonly used ML algorithms for Intrusion Detection:\n",
    "- Distance/Similarity Metrics: Manhattan Distance, Euclidean Distance, Mahalanobis Distance and their Scaled Versions.\n",
    "- Neural Networks\n",
    "- Support Vector Machines (One Class Variant)\n",
    "- Self-Organizing (Kohonen) Maps\n",
    "- Fuzzy Logic\n",
    "- Density Estimation\n",
    "- Subspace and Correlation based Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One Class Support Vector Machines\n",
    "\n",
    "- Trained on only one class of data. \n",
    "- Has to predict whether a new input belongs to the class.\n",
    "- Hyperparameters used:\n",
    " - $\\nu = \\{0.01, 0.05, 0.1\\}$\n",
    "      - Controls the number of Support Vectors and the approximate fraction of the training set misclassified (taken as outliers).\n",
    " - Kernel Function\n",
    "      - Used Linear and Radial Basis Function Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import Parser\n",
    "\n",
    "# For checking if file exists\n",
    "import os\n",
    "\n",
    "# For Data Analysis\n",
    "import pandas\n",
    "\n",
    "filename = \"Sample Data\"\n",
    "XMLStructureFilename = \"canstructure.xml\"\n",
    "raw_data = True\n",
    "\n",
    "df = pandas.DataFrame()\n",
    "\n",
    "if raw_data:\n",
    "    if not os.path.isfile(filename + \".parsed_raw_data\"):\n",
    "        df = Parser.get_raw_data(filename, XMLStructureFilename)\n",
    "        Parser.save_parsed_data_to_file(dataframe=df, filename=filename + \".parsed_raw_data\")\n",
    "    else:\n",
    "        df = Parser.load_parsed_data_from_file(filename + \".parsed_raw_data\")\n",
    "\n",
    "else:\n",
    "    if not os.path.isfile(filename + \".parsed_encoded_data\"):\n",
    "        df = Parser.get_label_encoded_data(filename)\n",
    "        Parser.save_parsed_data_to_file(dataframe=df, filename=filename + \".parsed_encoded_data\")\n",
    "    else:\n",
    "        df = Parser.load_parsed_data_from_file(filename + \".parsed_encoded_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import SVM\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "nonlinear_nu_point01 = joblib.load('Sample Data_RBF_nu=0.01.fitted_SVM_model')\n",
    "nonlinear_nu_point05 = joblib.load('Sample Data_RBF_nu=0.05.fitted_SVM_model')\n",
    "nonlinear_nu_point1 = joblib.load('Sample Data_RBF_nu=0.1.fitted_SVM_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import SVM\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "linear_nu_point01 = joblib.load('Sample Data_linear_nu=0.01.fitted_SVM_model')\n",
    "linear_nu_point05 = joblib.load('Sample Data_linear_nu=0.05.fitted_SVM_model')\n",
    "linear_nu_point1 = joblib.load('Sample Data_linear_nu=0.1.fitted_SVM_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import LSTMAnomalyDetector\n",
    "\n",
    "model25_10 = LSTMAnomalyDetector.LSTMAnomalyDetector(df, 25, 10, False)\n",
    "model25_10.load_weights('LSTMAD_(25, 10).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import SOM\n",
    "\n",
    "training_set_fraction = 0.7\n",
    "training_data = df.loc[:training_set_fraction * df.shape[0]]\n",
    "\n",
    "SOMModels = {3 : None, 5 : None, 7 : None, 10: None}\n",
    "\n",
    "def trainSOMs():\n",
    "    for k in SOMModels.keys():\n",
    "        som = SOM.trainSOM(training_data, k, k)\n",
    "        SOMModels[k] = som\n",
    "\n",
    "trainSOMs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "testing_set_fraction = 1 - training_set_fraction\n",
    "testing_data_non_anomalous =  df.loc[training_set_fraction * df.shape[0]:]\n",
    "\n",
    "import Noise\n",
    "import imp\n",
    "imp.reload(Noise)\n",
    "import pandas\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "testing_data_switched_hex = {0.1: [], 0.2: [], 0.4: [], 0.6: [], 0.8: []}\n",
    "for k in testing_data_switched_hex.keys():\n",
    "    df_t = df.loc[np.random.choice(range(df.shape[0]), size = testing_set_fraction * df.shape[0])]\n",
    "    testing_data_switched_hex[k].append(Noise.switch_hex(\n",
    "            df_t, prob=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:6: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "testing_data_with_gaussian_noised_timestamp = {(0, 0.01): [], (0, 0.05): [], (0, 0.1): [], \n",
    "                                               (0, 0.2): [], (0, 0.5): [], (0, 1): []}\n",
    "for k in testing_data_with_gaussian_noised_timestamp.keys():\n",
    "    df_t = df.loc[np.random.choice(range(df.shape[0]), size = testing_set_fraction * df.shape[0])]\n",
    "    testing_data_with_gaussian_noised_timestamp[k].append(Noise.add_gaussian_noise(\n",
    "            df_t, mean=k[0], variance=k[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nonlinear_nu_point01pred = nonlinear_nu_point01.predict(testing_data_non_anomalous)\n",
    "nonlinear_nu_point05pred = nonlinear_nu_point05.predict(testing_data_non_anomalous)\n",
    "nonlinear_nu_point1pred = nonlinear_nu_point1.predict(testing_data_non_anomalous)\n",
    "linear_nu_point01pred = linear_nu_point01.predict(testing_data_non_anomalous)\n",
    "linear_nu_point05pred = linear_nu_point05.predict(testing_data_non_anomalous)\n",
    "linear_nu_point1pred = linear_nu_point1.predict(testing_data_non_anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x154459160>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAAEACAYAAACUHkKwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzCQgv2nOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4ff278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion Matrices Illustration\n",
    "plt.imshow(confusion_matrix([1] * len(nonlinear_nu_point01pred), nonlinear_nu_point01pred, labels=[-1, 1]), interpolation='nearest', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel, nu = 0.01\n",
      "[[    0     0]\n",
      " [10735 25442]]\n",
      "RBF Kernel, nu = 0.05\n",
      "[[    0     0]\n",
      " [11284 24893]]\n",
      "RBF Kernel, nu = 0.1\n",
      "[[    0     0]\n",
      " [11560 24617]]\n"
     ]
    }
   ],
   "source": [
    "# Nonlinear Kernel Training\n",
    "print(\"RBF Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([1] * len(nonlinear_nu_point01pred), nonlinear_nu_point01pred, labels=[-1, 1]))\n",
    "print(\"RBF Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([1] * len(nonlinear_nu_point05pred), nonlinear_nu_point05pred, labels=[-1, 1]))\n",
    "print(\"RBF Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([1] * len(nonlinear_nu_point1pred), nonlinear_nu_point1pred, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel, nu = 0.01\n",
      "[[    0     0]\n",
      " [    0 36177]]\n",
      "Linear Kernel, nu = 0.05\n",
      "[[    0     0]\n",
      " [    0 36177]]\n",
      "Linear Kernel, nu = 0.1\n",
      "[[    0     0]\n",
      " [    0 36177]]\n"
     ]
    }
   ],
   "source": [
    "# Linear Kernel Training\n",
    "print(\"Linear Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([1] * len(linear_nu_point01pred), linear_nu_point01pred, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([1] * len(linear_nu_point05pred), linear_nu_point05pred, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([1] * len(linear_nu_point1pred), linear_nu_point1pred, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fuzzing (Testing Anomaly Detection)\n",
    "\n",
    "- The CAN packet data is in tne following format: UNIX_TIMESTAMP [16 Hex Digits]\n",
    "\n",
    "- We used two main methods for introducing anomalies: \n",
    " - Change a fraction of the Hex Digits to some random Hex Digit.\n",
    "      - Probabilities $p \\in \\{0.2, 0.4, 0.6, 0.8\\}$\n",
    " - Add Gaussian Noise to the Timestamp field.\n",
    "      -  $\\mu = 0$, $\\sigma \\in \\{0.01, 0.05, 0.1, 0.2, 0.5, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "nonlinear_nu_point01predAnomalousHex = nonlinear_nu_point01.predict(testing_data_switched_hex[0.1][0])\n",
    "nonlinear_nu_point05predAnomalousHex = nonlinear_nu_point05.predict(testing_data_switched_hex[0.1][0])\n",
    "nonlinear_nu_point1predAnomalousHex = nonlinear_nu_point1.predict(testing_data_switched_hex[0.1][0])\n",
    "linear_nu_point01predAnomalousHex = linear_nu_point01.predict(testing_data_switched_hex[0.1][0])\n",
    "linear_nu_point05predAnomalousHex = linear_nu_point05.predict(testing_data_switched_hex[0.1][0])\n",
    "linear_nu_point1predAnomalousHex = linear_nu_point1.predict(testing_data_switched_hex[0.1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3429 32748]\n",
      " [    0     0]]\n",
      "[[ 4496 31681]\n",
      " [    0     0]]\n",
      "[[ 5817 30360]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix([-1] * len(nonlinear_nu_point01predAnomalousHex), nonlinear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point05predAnomalousHex), nonlinear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point1predAnomalousHex), nonlinear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel, nu = 0.01\n",
      "[[  244 35933]\n",
      " [    0     0]]\n",
      "Linear Kernel, nu = 0.05\n",
      "[[ 1265 34912]\n",
      " [    0     0]]\n",
      "Linear Kernel, nu = 0.1\n",
      "[[ 2483 33694]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point01predAnomalousHex), linear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point05predAnomalousHex), linear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point1predAnomalousHex), linear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "nonlinear_nu_point01predAnomalousHex = nonlinear_nu_point01.predict(testing_data_switched_hex[0.2][0])\n",
    "nonlinear_nu_point05predAnomalousHex = nonlinear_nu_point05.predict(testing_data_switched_hex[0.2][0])\n",
    "nonlinear_nu_point1predAnomalousHex = nonlinear_nu_point1.predict(testing_data_switched_hex[0.2][0])\n",
    "linear_nu_point01predAnomalousHex = linear_nu_point01.predict(testing_data_switched_hex[0.2][0])\n",
    "linear_nu_point05predAnomalousHex = linear_nu_point05.predict(testing_data_switched_hex[0.2][0])\n",
    "linear_nu_point1predAnomalousHex = linear_nu_point1.predict(testing_data_switched_hex[0.2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel, nu = 0.01\n",
      "[[ 3429 32748]\n",
      " [    0     0]]\n",
      "RBF Kernel, nu = 0.05\n",
      "[[ 4496 31681]\n",
      " [    0     0]]\n",
      "RBF Kernel, nu = 0.1\n",
      "[[ 5817 30360]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Switch Hex Prob=0.2, NonLinear Kernel OCSVM\n",
    "print(\"RBF Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point01predAnomalousHex), nonlinear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"RBF Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point05predAnomalousHex), nonlinear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"RBF Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point1predAnomalousHex), nonlinear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel, nu = 0.01\n",
      "[[  244 35933]\n",
      " [    0     0]]\n",
      "Linear Kernel, nu = 0.05\n",
      "[[ 1265 34912]\n",
      " [    0     0]]\n",
      "Linear Kernel, nu = 0.1\n",
      "[[ 2483 33694]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Switch Hex Prob=0.2, NonLinear Kernel OCSVM\n",
    "print(\"Linear Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point01predAnomalousHex), linear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point05predAnomalousHex), linear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point1predAnomalousHex), linear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3548 32629]\n",
      " [    0     0]]\n",
      "[[ 4640 31537]\n",
      " [    0     0]]\n",
      "[[ 5942 30235]\n",
      " [    0     0]]\n",
      "[[  237 35940]\n",
      " [    0     0]]\n",
      "[[ 1223 34954]\n",
      " [    0     0]]\n",
      "[[ 2513 33664]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nonlinear_nu_point01predAnomalousHex = nonlinear_nu_point01.predict(testing_data_switched_hex[0.4][0])\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point01predAnomalousHex), nonlinear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "nonlinear_nu_point05predAnomalousHex = nonlinear_nu_point05.predict(testing_data_switched_hex[0.4][0])\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point05predAnomalousHex), nonlinear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "nonlinear_nu_point1predAnomalousHex = nonlinear_nu_point1.predict(testing_data_switched_hex[0.4][0])\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point1predAnomalousHex), nonlinear_nu_point1predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "linear_nu_point01predAnomalousHex = linear_nu_point01.predict(testing_data_switched_hex[0.4][0])\n",
    "print(confusion_matrix([-1] * len(linear_nu_point01predAnomalousHex), linear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "linear_nu_point05predAnomalousHex = linear_nu_point05.predict(testing_data_switched_hex[0.4][0])\n",
    "print(confusion_matrix([-1] * len(linear_nu_point05predAnomalousHex), linear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "linear_nu_point1predAnomalousHex = linear_nu_point1.predict(testing_data_switched_hex[0.4][0])\n",
    "print(confusion_matrix([-1] * len(linear_nu_point1predAnomalousHex), linear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3421 32756]\n",
      " [    0     0]]\n",
      "[[ 4609 31568]\n",
      " [    0     0]]\n",
      "[[ 5876 30301]\n",
      " [    0     0]]\n",
      "[[  245 35932]\n",
      " [    0     0]]\n",
      "[[ 1250 34927]\n",
      " [    0     0]]\n",
      "[[ 2545 33632]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nonlinear_nu_point01predAnomalousHex = nonlinear_nu_point01.predict(testing_data_switched_hex[0.6][0])\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point01predAnomalousHex), nonlinear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "nonlinear_nu_point05predAnomalousHex = nonlinear_nu_point05.predict(testing_data_switched_hex[0.6][0])\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point05predAnomalousHex), nonlinear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "nonlinear_nu_point1predAnomalousHex = nonlinear_nu_point1.predict(testing_data_switched_hex[0.6][0])\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point1predAnomalousHex), nonlinear_nu_point1predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "linear_nu_point01predAnomalousHex = linear_nu_point01.predict(testing_data_switched_hex[0.6][0])\n",
    "print(confusion_matrix([-1] * len(linear_nu_point01predAnomalousHex), linear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "linear_nu_point05predAnomalousHex = linear_nu_point05.predict(testing_data_switched_hex[0.6][0])\n",
    "print(confusion_matrix([-1] * len(linear_nu_point05predAnomalousHex), linear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "\n",
    "linear_nu_point1predAnomalousHex = linear_nu_point1.predict(testing_data_switched_hex[0.6][0])\n",
    "print(confusion_matrix([-1] * len(linear_nu_point1predAnomalousHex), linear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "nonlinear_nu_point01predAnomalousHex = nonlinear_nu_point01.predict(testing_data_switched_hex[0.8][0])\n",
    "nonlinear_nu_point05predAnomalousHex = nonlinear_nu_point05.predict(testing_data_switched_hex[0.8][0])\n",
    "nonlinear_nu_point1predAnomalousHex = nonlinear_nu_point1.predict(testing_data_switched_hex[0.8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "linear_nu_point01predAnomalousHex = linear_nu_point01.predict(testing_data_switched_hex[0.8][0])\n",
    "linear_nu_point05predAnomalousHex = linear_nu_point05.predict(testing_data_switched_hex[0.8][0])\n",
    "linear_nu_point1predAnomalousHex = linear_nu_point1.predict(testing_data_switched_hex[0.8][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher Probabilities - Intution?\n",
    "\n",
    "- Easier?: Increasing the number of Hex Digits that are changed should ideally make an Anomaly Detection easier.\n",
    "- Harder?: Changing too many Hex Digits can cause the packet to seem valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel, nu = 0.01\n",
      "[[ 3429 32748]\n",
      " [    0     0]]\n",
      "RBF Kernel, nu = 0.05\n",
      "[[ 4496 31681]\n",
      " [    0     0]]\n",
      "RBF Kernel, nu = 0.1\n",
      "[[ 5817 30360]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Switch Hex Prob=0.8, NonLinear Kernel OCSVM\n",
    "print(\"RBF Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point01predAnomalousHex), nonlinear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"RBF Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point05predAnomalousHex), nonlinear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"RBF Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([-1] * len(nonlinear_nu_point1predAnomalousHex), nonlinear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel, nu = 0.01\n",
      "[[  244 35933]\n",
      " [    0     0]]\n",
      "Linear Kernel, nu = 0.05\n",
      "[[ 1265 34912]\n",
      " [    0     0]]\n",
      "Linear Kernel, nu = 0.1\n",
      "[[ 2483 33694]\n",
      " [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Switch Hex Prob=0.2, Linear Kernel OCSVM\n",
    "print(\"Linear Kernel, nu = 0.01\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point01predAnomalousHex), linear_nu_point01predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.05\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point05predAnomalousHex), linear_nu_point05predAnomalousHex, labels=[-1, 1]))\n",
    "print(\"Linear Kernel, nu = 0.1\")\n",
    "print(confusion_matrix([-1] * len(linear_nu_point1predAnomalousHex), linear_nu_point1predAnomalousHex, labels=[-1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OCSVM Results Summary (1)\n",
    "- Similar results on Gaussian Noised timestamps.\n",
    "- Linear Kernels works quite well.\n",
    "- Many ML researchers such as Andrew Ng. have suggested that linear kernels should be used when there are a lot of datapoints.\n",
    "    - Nonlinear Kernels generally take a long time to optimize when there are many datapoints (the training set used contains approximately 130,000 points), tolerance not low enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OCSVM Results Summary (2)\n",
    "\n",
    "- Quite good results! With the Linear Kernel with $\\nu = 0.01$:\n",
    " - 0.07369264557% false positive rate  when probability = 0.2 is used for Hex Switching, and, \n",
    " - 0.103044452% false positive rate when using Gaussian Noise with $\\sigma = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-Organizing Map (Kohonen Map)\n",
    "\n",
    "- Special Type of Neural Network.\n",
    "- Can be thought of as a Fully Connected Grid that performs dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](SOM_IMAGES/Cue_Distance_Heatmap_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance with 3x3 SOM for training set: 22.5831233731\n",
      "Average Distance with 5x5 SOM for training set: 28.3443649242\n",
      "Average Distance with 7x7 SOM for training set: 31.1494448656\n"
     ]
    }
   ],
   "source": [
    "### Self-Organizing Map (Kohonen Map)\n",
    "\n",
    "import SOM\n",
    "\n",
    "print(\"Average Distance with 3x3 SOM for training set: \" + str(np.average(SOMModels[3].distances(training_data))))\n",
    "print(\"Average Distance with 5x5 SOM for training set: \" + str(np.average(SOMModels[5].distances(training_data))))\n",
    "print(\"Average Distance with 7x7 SOM for training set: \" + str(np.average(SOMModels[7].distances(training_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance with 3x3 SOM for non-anomalous testing set: 131.399610135\n",
      "Average Distance with 5x5 SOM for non-anomalous testing set: 128.593592325\n",
      "Average Distance with 7x7 SOM for non-anomalous testing set: 127.411091128\n"
     ]
    }
   ],
   "source": [
    "# Test Data without Anomalies\n",
    "\n",
    "print(\"Average Distance with 3x3 SOM for non-anomalous testing set: \" + \n",
    "      str(np.average(SOMModels[3].distances(testing_data_non_anomalous))))\n",
    "print(\"Average Distance with 5x5 SOM for non-anomalous testing set: \" + \n",
    "      str(np.average(SOMModels[5].distances(testing_data_non_anomalous))))\n",
    "print(\"Average Distance with 7x7 SOM for non-anomalous testing set: \" + \n",
    "          str(np.average(SOMModels[7].distances(testing_data_non_anomalous))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_non_anomalous) - n_prev)):\n",
    "    docX.append(testing_data_non_anomalous.iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_non_anomalous.iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_non_anomalous = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_switched_hex[0.1][0]) - n_prev)):\n",
    "    docX.append(testing_data_switched_hex[0.1][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_switched_hex[0.1][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "predictions_switched_hex_point1 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_switched_hex[0.2][0]) - n_prev)):\n",
    "    docX.append(testing_data_switched_hex[0.2][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_switched_hex[0.2][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_switched_hex_point2 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_switched_hex[0.4][0]) - n_prev)):\n",
    "    docX.append(testing_data_switched_hex[0.4][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_switched_hex[0.4][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_switched_hex_point4 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_switched_hex[0.6][0]) - n_prev)):\n",
    "    docX.append(testing_data_switched_hex[0.6][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_switched_hex[0.6][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_switched_hex_point6 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_switched_hex[0.8][0]) - n_prev)):\n",
    "    docX.append(testing_data_switched_hex[0.8][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_switched_hex[0.8][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_switched_hex_point8 = model25_10.predict(np.array(alsX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_with_gaussian_noised_timestamp[(0, 0.01)][0]) - n_prev)):\n",
    "    docX.append(testing_data_with_gaussian_noised_timestamp[(0, 0.01)][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_with_gaussian_noised_timestamp[(0, 0.01)][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_gauss_noise_point01 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_with_gaussian_noised_timestamp[(0, 0.05)][0]) - n_prev)):\n",
    "    docX.append(testing_data_with_gaussian_noised_timestamp[(0, 0.05)][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_with_gaussian_noised_timestamp[(0, 0.05)][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_gauss_noise_point05 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_with_gaussian_noised_timestamp[(0, 0.1)][0]) - n_prev)):\n",
    "    docX.append(testing_data_with_gaussian_noised_timestamp[(0, 0.1)][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_with_gaussian_noised_timestamp[(0, 0.1)][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_gauss_noise_point1 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_with_gaussian_noised_timestamp[(0, 0.5)][0]) - n_prev)):\n",
    "    docX.append(testing_data_with_gaussian_noised_timestamp[(0, 0.5)][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_with_gaussian_noised_timestamp[(0, 0.5)][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_gauss_noise_point5 = model25_10.predict(np.array(alsX))\n",
    "\n",
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_with_gaussian_noised_timestamp[(0, 1)][0]) - n_prev)):\n",
    "    docX.append(testing_data_with_gaussian_noised_timestamp[(0, 1)][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_with_gaussian_noised_timestamp[(0, 1)][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_gauss_noise_1 = model25_10.predict(np.array(alsX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10764/62996 [====>.........................] - ETA: 697s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-4b00a432180d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0malsY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredictions_gauss_noise_point2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel25_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malsX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malsY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Valli/Library/Python/3.5/lib/python/site-packages/keras/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, X, y, batch_size, show_accuracy, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Valli/Library/Python/3.5/lib/python/site-packages/keras/models.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Valli/Library/Python/3.5/lib/python/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 428\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docX, docY = [], []\n",
    "n_prev = 25\n",
    "for i in tqdm(range(len(testing_data_with_gaussian_noised_timestamp[(0, 0.2)][0]) - n_prev)):\n",
    "    docX.append(testing_data_with_gaussian_noised_timestamp[(0, 0.2)][0].iloc[i:i+n_prev].as_matrix())\n",
    "    docY.append(testing_data_with_gaussian_noised_timestamp[(0, 0.2)][0].iloc[i+n_prev].as_matrix())\n",
    "alsX = np.array(docX)\n",
    "alsY = np.array(docY)\n",
    "\n",
    "predictions_gauss_noise_point2 = model25_10.evaluate(np.array(alsX), np.array(alsY), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Sequences: Long Short Term Memory Anomaly Detection\n",
    "\n",
    "- Neural Network that accepts a sequence of data and outputs a sequence of data.\n",
    "- Hyperparameters Used: \n",
    "     - Number of Unrolling Timesteps $\\in \\{25, 50\\}$\n",
    "     - Number of Neurons in the single Hidden Layer $\\in \\{10, 20, 50\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Image](1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding Noise when using Sequences\n",
    "\n",
    "- For testing Anomaly Detection for LSTMs, we introduced Gaussian Noise in the timestamps based on an exponential distribution with scale parameter $\\beta = 100$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM Experiments and Results\n",
    "\n",
    "- 100 packets were passed as input with a sliding window.\n",
    "- 25 Last Packets with a hidden layer of 10 units with an error rate with the exponentially distributed noise gives the best false positive error rate of 0.035%\n",
    "- Sequences help!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
